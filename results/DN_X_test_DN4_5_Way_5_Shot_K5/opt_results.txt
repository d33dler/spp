Namespace(arch='DN_X', config='../models/architectures/configs/DN4_DA.yaml', dengine=False, refit_dengine=False, dataset_dir='../dataset/miniImageNet', data_name='test', mode='test', resume='', epochs=1, ngpu=1, print_freq=100, outf='../results/DN_X_test_DN4_5_Way_5_Shot_K5')DN_X(
  (BACKBONE): FourLayer_64F(
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.2, inplace=True)
      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (6): LeakyReLU(negative_slope=0.2, inplace=True)
      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (10): LeakyReLU(negative_slope=0.2, inplace=True)
      (11): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (13): LeakyReLU(negative_slope=0.2, inplace=True)
    )
    (criterion): CrossEntropyLoss()
  )
  (DE): RandomForestHead()
)
===================================== Round 0 =====================================
Testset: 600-------------0
Test-(0): [100/600]	Time 0.127 (0.134)	Loss 16.860 (10.971)	Prec@1 34.0 (38.138614654541016)
Namespace(arch='DN_X', config='../models/architectures/configs/DN4_DA.yaml', dengine=False, refit_dengine=False, dataset_dir='../dataset/miniImageNet', data_name='test', mode='test', resume='../results/models/dn4__miniImageNet_epoch_23.pth.tar', epochs=1, ngpu=1, print_freq=100, outf='../results/DN_X_test_DN4_5_Way_5_Shot_K5')=> loaded checkpoint '../results/models/dn4__miniImageNet_epoch_23.pth.tar' (epoch 23)
DN_X(
  (BACKBONE): FourLayer_64F(
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.2, inplace=True)
      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (6): LeakyReLU(negative_slope=0.2, inplace=True)
      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (10): LeakyReLU(negative_slope=0.2, inplace=True)
      (11): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (13): LeakyReLU(negative_slope=0.2, inplace=True)
    )
    (criterion): CrossEntropyLoss()
  )
  (DE): RandomForestHead()
)
===================================== Round 0 =====================================
Testset: 600-------------0
Test-(23): [100/600]	Time 0.111 (0.141)	Loss 1.610 (1.352)	Prec@1 60.0 (65.20791625976562)

Test-(23): [200/600]	Time 0.122 (0.129)	Loss 2.594 (1.367)	Prec@1 40.0 (65.13433074951172)

Test-(23): [300/600]	Time 0.103 (0.128)	Loss 2.227 (1.372)	Prec@1 48.0 (64.77075958251953)

Test-(23): [400/600]	Time 0.103 (0.124)	Loss 1.645 (1.391)	Prec@1 64.0 (64.48877716064453)

Test-(23): [500/600]	Time 0.106 (0.122)	Loss 0.921 (1.405)	Prec@1 70.0 (64.3113784790039)
 * Prec@1 64.017 Best_prec1 0.000Test accuracy= 64.01667022705078 h= 0.734659731388092
===================================== Round 1 =====================================
Testset: 600-------------1
Test-(23): [100/600]	Time 0.130 (0.123)	Loss 1.434 (1.388)	Prec@1 62.0 (64.13861083984375)

Test-(23): [200/600]	Time 0.105 (0.118)	Loss 2.129 (1.397)	Prec@1 52.0 (64.16915893554688)

Test-(23): [300/600]	Time 0.127 (0.117)	Loss 1.141 (1.385)	Prec@1 58.0 (64.5049819946289)

Test-(23): [400/600]	Time 0.114 (0.116)	Loss 2.272 (1.376)	Prec@1 60.0 (64.77306365966797)

Test-(23): [500/600]	Time 0.106 (0.115)	Loss 1.470 (1.381)	Prec@1 62.0 (64.4990005493164)
 * Prec@1 64.427 Best_prec1 64.017Test accuracy= 64.42666625976562 h= 0.7248280048370361
===================================== Round 2 =====================================
Testset: 600-------------2
Test-(23): [100/600]	Time 0.101 (0.128)	Loss 2.258 (1.487)	Prec@1 58.0 (62.297027587890625)

Test-(23): [200/600]	Time 0.132 (0.122)	Loss 0.674 (1.448)	Prec@1 76.0 (63.31343460083008)

Test-(23): [300/600]	Time 0.121 (0.120)	Loss 1.285 (1.452)	Prec@1 66.0 (63.63454818725586)

Test-(23): [400/600]	Time 0.106 (0.119)	Loss 1.422 (1.445)	Prec@1 68.0 (63.8204460144043)

Test-(23): [500/600]	Time 0.113 (0.119)	Loss 1.869 (1.444)	Prec@1 60.0 (63.80838394165039)
 * Prec@1 63.863 Best_prec1 64.427Test accuracy= 63.86333465576172 h= 0.7157545685768127
===================================== Round 3 =====================================
Testset: 600-------------3
Test-(23): [100/600]	Time 0.132 (0.128)	Loss 1.171 (1.484)	Prec@1 68.0 (63.049503326416016)

Test-(23): [200/600]	Time 0.117 (0.120)	Loss 1.654 (1.440)	Prec@1 66.0 (64.31841278076172)

Test-(23): [300/600]	Time 0.103 (0.118)	Loss 1.853 (1.418)	Prec@1 50.0 (64.63787078857422)

Test-(23): [400/600]	Time 0.093 (0.117)	Loss 1.721 (1.416)	Prec@1 46.0 (64.6733169555664)

Test-(23): [500/600]	Time 0.106 (0.117)	Loss 1.211 (1.412)	Prec@1 62.0 (64.68263244628906)
 * Prec@1 64.540 Best_prec1 64.427Test accuracy= 64.54000091552734 h= 0.7272393703460693
===================================== Round 4 =====================================
Testset: 600-------------4
Test-(23): [100/600]	Time 0.100 (0.129)	Loss 1.287 (1.439)	Prec@1 64.0 (62.95049285888672)

Test-(23): [200/600]	Time 0.132 (0.123)	Loss 1.473 (1.431)	Prec@1 64.0 (63.164180755615234)

Test-(23): [300/600]	Time 0.115 (0.121)	Loss 2.145 (1.427)	Prec@1 58.0 (63.62790298461914)

Test-(23): [400/600]	Time 0.132 (0.120)	Loss 1.157 (1.425)	Prec@1 62.0 (63.725685119628906)

Test-(23): [500/600]	Time 0.123 (0.120)	Loss 2.019 (1.430)	Prec@1 60.0 (63.648704528808594)
 * Prec@1 63.607 Best_prec1 64.540Test accuracy= 63.606666564941406 h= 0.7396629452705383

Aver_accuracy= 64.09066772460938 Aver_h= 0.7284289240837097
Namespace(arch='DN_X', config='../models/architectures/configs/DN4_DA.yaml', dengine=True, refit_dengine=True, dataset_dir='../dataset/miniImageNet', data_name='test', mode='test', resume='../results/models/dn4__miniImageNet_epoch_23.pth.tar', epochs=1, ngpu=1, print_freq=100, outf='../results/DN_X_test_DN4_5_Way_5_Shot_K5')=> loaded checkpoint '../results/models/dn4__miniImageNet_epoch_23.pth.tar' (epoch 23)
DN_X(
  (BACKBONE): FourLayer_64F(
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.2, inplace=True)
      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (6): LeakyReLU(negative_slope=0.2, inplace=True)
      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (10): LeakyReLU(negative_slope=0.2, inplace=True)
      (11): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (13): LeakyReLU(negative_slope=0.2, inplace=True)
    )
    (criterion): CrossEntropyLoss()
  )
  (DE): RandomForestHead()
)
Trainset: 10000
Valset: 800
Testset: 600
Namespace(arch='DN_X', config='../models/architectures/configs/DN4_DA.yaml', dengine=True, refit_dengine=True, dataset_dir='../dataset/miniImageNet', data_name='test', mode='test', resume='../results/models/dn4__miniImageNet_epoch_23.pth.tar', epochs=1, ngpu=1, print_freq=100, outf='../results/DN_X_test_DN4_5_Way_5_Shot_K5')=> loaded checkpoint '../results/models/dn4__miniImageNet_epoch_23.pth.tar' (epoch 23)
DN_X(
  (BACKBONE): FourLayer_64F(
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.2, inplace=True)
      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (6): LeakyReLU(negative_slope=0.2, inplace=True)
      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (10): LeakyReLU(negative_slope=0.2, inplace=True)
      (11): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (13): LeakyReLU(negative_slope=0.2, inplace=True)
    )
    (criterion): CrossEntropyLoss()
  )
  (DE): RandomForestHead()
)
Trainset: 10000
Valset: 800
Testset: 600
===================================== Round 0 =====================================
Testset: 600-------------0
Test-(23): [100/600]	Time 0.109 (0.142)	Loss 1.285 (1.259)	Prec@1 62.0 (64.61386108398438)

Test-(23): [200/600]	Time 0.148 (0.135)	Loss 1.165 (1.255)	Prec@1 74.0 (64.9353256225586)

Test-(23): [300/600]	Time 0.126 (0.135)	Loss 1.285 (1.263)	Prec@1 62.0 (64.18604278564453)

Test-(23): [400/600]	Time 0.122 (0.139)	Loss 1.245 (1.263)	Prec@1 66.0 (64.16957092285156)

Test-(23): [500/600]	Time 0.152 (0.139)	Loss 1.285 (1.264)	Prec@1 62.0 (64.11975860595703)
 * Prec@1 64.033 Best_prec1 0.000Test accuracy= 64.03333282470703 h= 0.7131599187850952
===================================== Round 1 =====================================
Testset: 600-------------1
Test-(23): [100/600]	Time 0.118 (0.138)	Loss 1.185 (1.261)	Prec@1 72.0 (64.33663177490234)

Test-(23): [200/600]	Time 0.177 (0.133)	Loss 1.405 (1.254)	Prec@1 50.0 (65.07463073730469)

Test-(23): [300/600]	Time 0.143 (0.132)	Loss 1.365 (1.257)	Prec@1 54.0 (64.78404998779297)

Test-(23): [400/600]	Time 0.123 (0.130)	Loss 1.225 (1.262)	Prec@1 68.0 (64.28428649902344)

Test-(23): [500/600]	Time 0.105 (0.130)	Loss 1.245 (1.266)	Prec@1 66.0 (63.85628890991211)
 * Prec@1 63.980 Best_prec1 64.033Test accuracy= 63.97999954223633 h= 0.7308217883110046
===================================== Round 2 =====================================
Testset: 600-------------2
Test-(23): [100/600]	Time 0.107 (0.137)	Loss 1.185 (1.273)	Prec@1 72.0 (63.22772216796875)

Test-(23): [200/600]	Time 0.122 (0.133)	Loss 1.065 (1.271)	Prec@1 84.0 (63.353233337402344)

Test-(23): [300/600]	Time 0.143 (0.131)	Loss 1.425 (1.269)	Prec@1 48.0 (63.588035583496094)

Test-(23): [400/600]	Time 0.156 (0.129)	Loss 1.205 (1.267)	Prec@1 70.0 (63.81047058105469)

Test-(23): [500/600]	Time 0.108 (0.130)	Loss 1.305 (1.264)	Prec@1 60.0 (64.1077880859375)
 * Prec@1 64.090 Best_prec1 64.033Test accuracy= 64.08999633789062 h= 0.7727298736572266
===================================== Round 3 =====================================
Testset: 600-------------3
Test-(23): [100/600]	Time 0.126 (0.142)	Loss 1.145 (1.276)	Prec@1 76.0 (62.89108657836914)

Test-(23): [200/600]	Time 0.135 (0.135)	Loss 1.245 (1.267)	Prec@1 66.0 (63.830848693847656)

Test-(23): [300/600]	Time 0.134 (0.137)	Loss 1.305 (1.268)	Prec@1 60.0 (63.707637786865234)

Test-(23): [400/600]	Time 0.123 (0.135)	Loss 1.245 (1.266)	Prec@1 66.0 (63.860347747802734)

Test-(23): [500/600]	Time 0.109 (0.133)	Loss 1.305 (1.265)	Prec@1 60.0 (63.94810485839844)
 * Prec@1 64.167 Best_prec1 64.090Test accuracy= 64.16666412353516 h= 0.6723872423171997
===================================== Round 4 =====================================
Testset: 600-------------4
Test-(23): [100/600]	Time 0.113 (0.139)	Loss 1.185 (1.276)	Prec@1 72.0 (62.87128448486328)

Test-(23): [200/600]	Time 0.139 (0.134)	Loss 1.205 (1.266)	Prec@1 70.0 (63.840797424316406)

Test-(23): [300/600]	Time 0.116 (0.131)	Loss 1.225 (1.263)	Prec@1 68.0 (64.15282440185547)

Test-(23): [400/600]	Time 0.139 (0.131)	Loss 1.265 (1.266)	Prec@1 64.0 (63.895259857177734)

Test-(23): [500/600]	Time 0.138 (0.131)	Loss 1.445 (1.266)	Prec@1 46.0 (63.8403205871582)
 * Prec@1 63.860 Best_prec1 64.167Test accuracy= 63.86000061035156 h= 0.6846365332603455

Aver_accuracy= 64.0260009765625 Aver_h= 0.7147470712661743
Namespace(arch='DN_X', config='../models/architectures/configs/DN4_DA.yaml', dengine=True, refit_dengine=True, dataset_dir='../dataset/miniImageNet', data_name='test', mode='test', resume='../results/models/dn4__miniImageNet_epoch_23.pth.tar', epochs=1, ngpu=1, print_freq=100, outf='../results/DN_X_test_DN4_5_Way_5_Shot_K5')=> loaded checkpoint '../results/models/dn4__miniImageNet_epoch_23.pth.tar' (epoch 23)
DN_X(
  (BACKBONE): FourLayer_64F(
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.2, inplace=True)
      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (6): LeakyReLU(negative_slope=0.2, inplace=True)
      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (10): LeakyReLU(negative_slope=0.2, inplace=True)
      (11): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (13): LeakyReLU(negative_slope=0.2, inplace=True)
    )
    (criterion): CrossEntropyLoss()
  )
  (DE): XGBHead()
)
Trainset: 10000
Valset: 800
Testset: 600
Namespace(arch='DN_X', config='../models/architectures/configs/DN4_DA.yaml', dengine=True, refit_dengine=True, dataset_dir='../dataset/miniImageNet', data_name='test', mode='test', resume='../results/models/dn4__miniImageNet_epoch_23.pth.tar', epochs=1, ngpu=1, print_freq=100, outf='../results/DN_X_test_DN4_5_Way_5_Shot_K5')=> loaded checkpoint '../results/models/dn4__miniImageNet_epoch_23.pth.tar' (epoch 23)
DN_X(
  (BACKBONE): FourLayer_64F(
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.2, inplace=True)
      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (6): LeakyReLU(negative_slope=0.2, inplace=True)
      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (10): LeakyReLU(negative_slope=0.2, inplace=True)
      (11): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (13): LeakyReLU(negative_slope=0.2, inplace=True)
    )
    (criterion): CrossEntropyLoss()
  )
  (DE): XGBHead()
)
Trainset: 10000
Valset: 800
Testset: 600
===================================== Round 0 =====================================
Testset: 600-------------0
Test-(23): [100/600]	Time 0.126 (0.138)	Loss 1.145 (1.268)	Prec@1 76.0 (63.70296859741211)

Test-(23): [200/600]	Time 0.132 (0.135)	Loss 1.245 (1.273)	Prec@1 66.0 (63.13433074951172)

Test-(23): [300/600]	Time 0.114 (0.133)	Loss 1.225 (1.270)	Prec@1 68.0 (63.49501419067383)
Namespace(arch='DN_X', config='../models/architectures/configs/DN4_DA.yaml', dengine=True, refit_dengine=True, dataset_dir='../dataset/miniImageNet', data_name='test', mode='test', resume='../results/models/dn4__miniImageNet_epoch_23.pth.tar', epochs=1, ngpu=1, print_freq=100, outf='../results/DN_X_test_DN4_5_Way_5_Shot_K5')=> loaded checkpoint '../results/models/dn4__miniImageNet_epoch_23.pth.tar' (epoch 23)
DN_X(
  (BACKBONE): FourLayer_64F(
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.2, inplace=True)
      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (6): LeakyReLU(negative_slope=0.2, inplace=True)
      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (10): LeakyReLU(negative_slope=0.2, inplace=True)
      (11): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (13): LeakyReLU(negative_slope=0.2, inplace=True)
    )
    (criterion): CrossEntropyLoss()
  )
  (DE): XGBHead()
)
Trainset: 10000
Valset: 800
Testset: 600
===================================== Round 0 =====================================
Testset: 600-------------0
Test-(23): [100/600]	Time 0.136 (0.157)	Loss 1.225 (1.261)	Prec@1 68.0 (64.35643768310547)

Test-(23): [200/600]	Time 0.155 (0.153)	Loss 1.305 (1.268)	Prec@1 60.0 (63.72139358520508)

Test-(23): [300/600]	Time 0.138 (0.151)	Loss 1.165 (1.268)	Prec@1 74.0 (63.674415588378906)

Test-(23): [400/600]	Time 0.158 (0.150)	Loss 1.245 (1.273)	Prec@1 66.0 (63.20698165893555)

Test-(23): [500/600]	Time 0.147 (0.149)	Loss 1.145 (1.271)	Prec@1 76.0 (63.353294372558594)
 * Prec@1 63.337 Best_prec1 0.000Test accuracy= 63.336666107177734 h= 0.7185916900634766
===================================== Round 1 =====================================
Testset: 600-------------1
Test-(23): [100/600]	Time 0.143 (0.157)	Loss 1.365 (1.284)	Prec@1 54.0 (62.118812561035156)

Test-(23): [200/600]	Time 0.141 (0.152)	Loss 1.265 (1.269)	Prec@1 64.0 (63.60198974609375)

Test-(23): [300/600]	Time 0.140 (0.150)	Loss 1.365 (1.268)	Prec@1 54.0 (63.687705993652344)

Test-(23): [400/600]	Time 0.143 (0.150)	Loss 1.225 (1.267)	Prec@1 68.0 (63.8204460144043)

Test-(23): [500/600]	Time 0.143 (0.150)	Loss 1.405 (1.265)	Prec@1 50.0 (63.96806335449219)
 * Prec@1 64.017 Best_prec1 63.337Test accuracy= 64.01667022705078 h= 0.7376336455345154
===================================== Round 2 =====================================
Testset: 600-------------2
Test-(23): [100/600]	Time 0.138 (0.156)	Loss 1.285 (1.274)	Prec@1 62.0 (63.108909606933594)

Test-(23): [200/600]	Time 0.194 (0.152)	Loss 1.245 (1.269)	Prec@1 66.0 (63.58209228515625)

Test-(23): [300/600]	Time 0.138 (0.152)	Loss 1.245 (1.265)	Prec@1 66.0 (64.00664520263672)

Test-(23): [400/600]	Time 0.157 (0.151)	Loss 1.405 (1.267)	Prec@1 50.0 (63.745635986328125)

Test-(23): [500/600]	Time 0.138 (0.149)	Loss 1.345 (1.267)	Prec@1 56.0 (63.792415618896484)
 * Prec@1 63.920 Best_prec1 64.017Test accuracy= 63.91999816894531 h= 0.6985398530960083
===================================== Round 3 =====================================
Testset: 600-------------3
Test-(23): [100/600]	Time 0.131 (0.159)	Loss 1.185 (1.262)	Prec@1 72.0 (64.23762512207031)

Test-(23): [200/600]	Time 0.154 (0.154)	Loss 1.325 (1.266)	Prec@1 58.0 (63.93035125732422)

Test-(23): [300/600]	Time 0.140 (0.154)	Loss 1.125 (1.269)	Prec@1 78.0 (63.59468078613281)

Test-(23): [400/600]	Time 0.195 (0.153)	Loss 1.225 (1.269)	Prec@1 68.0 (63.59600830078125)

Test-(23): [500/600]	Time 0.158 (0.153)	Loss 1.385 (1.270)	Prec@1 52.0 (63.46906280517578)
 * Prec@1 63.417 Best_prec1 64.017Test accuracy= 63.41666793823242 h= 0.744555652141571
===================================== Round 4 =====================================
Testset: 600-------------4
Test-(23): [100/600]	Time 0.161 (0.168)	Loss 1.125 (1.275)	Prec@1 78.0 (62.95049285888672)

Test-(23): [200/600]	Time 0.142 (0.159)	Loss 1.145 (1.266)	Prec@1 76.0 (63.93035125732422)

Test-(23): [300/600]	Time 0.198 (0.155)	Loss 1.285 (1.266)	Prec@1 62.0 (63.8604621887207)

Test-(23): [400/600]	Time 0.135 (0.153)	Loss 1.345 (1.269)	Prec@1 56.0 (63.54114532470703)

Test-(23): [500/600]	Time 0.140 (0.152)	Loss 1.225 (1.271)	Prec@1 68.0 (63.41716766357422)
 * Prec@1 63.350 Best_prec1 64.017Test accuracy= 63.349998474121094 h= 0.7342115640640259

Aver_accuracy= 63.608001708984375 Aver_h= 0.7267064809799194
Namespace(arch='DN_X', config='../models/architectures/configs/DN7_DA.yaml', dengine=True, refit_dengine=True, dataset_dir='../dataset/StanfordDogs/', data_name='test', mode='test', resume='../results/models/dn7da_stanfordogs_5_5_k3_epoch_27.pth.tar', epochs=1, ngpu=1, print_freq=100, outf='../results/DN_X_test_DN4_5_Way_5_Shot_K5')=> loaded checkpoint '../results/models/dn7da_stanfordogs_5_5_k3_epoch_27.pth.tar' (epoch 27)
DN_X(
  (BACKBONE): SevenLayer_64F(
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.2, inplace=True)
      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (6): LeakyReLU(negative_slope=0.2, inplace=True)
      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (10): LeakyReLU(negative_slope=0.2, inplace=True)
      (11): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (13): LeakyReLU(negative_slope=0.2, inplace=True)
      (14): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (15): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (16): LeakyReLU(negative_slope=0.2, inplace=True)
      (17): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (18): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (19): LeakyReLU(negative_slope=0.2, inplace=True)
      (20): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (21): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (22): LeakyReLU(negative_slope=0.2, inplace=True)
      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (criterion): CrossEntropyLoss()
  )
  (DE): XGBHead()
)
Trainset: 10000
Valset: 600
Testset: 600
===================================== Round 0 =====================================
Testset: 600-------------0
Test-(27): [100/600]	Time 0.157 (0.163)	Loss 1.045 (1.246)	Prec@1 86.0 (65.92079162597656)
Namespace(arch='DN_X', config='../models/architectures/configs/DN7_DA.yaml', dengine=True, refit_dengine=True, dataset_dir='../dataset/StanfordDogs/', data_name='test', mode='test', resume='../results/models/dn7da_stanfordogs_5_5_k3_epoch_27.pth.tar', epochs=1, ngpu=1, print_freq=100, outf='../results/DN_X_test_DN4_5_Way_5_Shot_K5')=> loaded checkpoint '../results/models/dn7da_stanfordogs_5_5_k3_epoch_27.pth.tar' (epoch 27)
DN_X(
  (BACKBONE): SevenLayer_64F(
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.2, inplace=True)
      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (6): LeakyReLU(negative_slope=0.2, inplace=True)
      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (10): LeakyReLU(negative_slope=0.2, inplace=True)
      (11): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (13): LeakyReLU(negative_slope=0.2, inplace=True)
      (14): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (15): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (16): LeakyReLU(negative_slope=0.2, inplace=True)
      (17): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (18): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (19): LeakyReLU(negative_slope=0.2, inplace=True)
      (20): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (21): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (22): LeakyReLU(negative_slope=0.2, inplace=True)
      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (criterion): CrossEntropyLoss()
  )
  (DE): XGBHead()
)
Trainset: 10000
Valset: 600
Testset: 600
===================================== Round 0 =====================================
Testset: 600-------------0
Test-(27): [100/600]	Time 0.139 (0.159)	Loss 1.165 (1.244)	Prec@1 74.0 (66.07920837402344)

Test-(27): [200/600]	Time 0.204 (0.158)	Loss 1.225 (1.240)	Prec@1 68.0 (66.43781280517578)

Test-(27): [300/600]	Time 0.164 (0.159)	Loss 1.325 (1.242)	Prec@1 58.0 (66.24584197998047)
Namespace(arch='DN_X', config='../models/architectures/configs/DN7_DA.yaml', dengine=True, refit_dengine=True, dataset_dir='../dataset/StanfordDogs/', data_name='test', mode='test', resume='../results/models/dn7da_stanfordogs_5_5_k3_epoch_27.pth.tar', epochs=1, ngpu=1, print_freq=100, outf='../results/DN_X_test_DN4_5_Way_5_Shot_K5')=> loaded checkpoint '../results/models/dn7da_stanfordogs_5_5_k3_epoch_27.pth.tar' (epoch 27)
DN_X(
  (BACKBONE): SevenLayer_64F(
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.2, inplace=True)
      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (6): LeakyReLU(negative_slope=0.2, inplace=True)
      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (10): LeakyReLU(negative_slope=0.2, inplace=True)
      (11): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (13): LeakyReLU(negative_slope=0.2, inplace=True)
      (14): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (15): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (16): LeakyReLU(negative_slope=0.2, inplace=True)
      (17): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (18): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (19): LeakyReLU(negative_slope=0.2, inplace=True)
      (20): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (21): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (22): LeakyReLU(negative_slope=0.2, inplace=True)
      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (criterion): CrossEntropyLoss()
  )
  (DE): XGBHead()
)
Trainset: 10000
Valset: 600
Testset: 600
===================================== Round 0 =====================================
Testset: 600-------------0
Test-(27): [100/600]	Time 0.134 (0.163)	Loss 1.105 (1.240)	Prec@1 80.0 (66.47525024414062)

Test-(27): [200/600]	Time 0.163 (0.158)	Loss 1.245 (1.240)	Prec@1 66.0 (66.5273666381836)

Test-(27): [300/600]	Time 0.146 (0.158)	Loss 1.165 (1.241)	Prec@1 74.0 (66.4186019897461)

Test-(27): [400/600]	Time 0.159 (0.158)	Loss 1.185 (1.237)	Prec@1 72.0 (66.79301452636719)

Test-(27): [500/600]	Time 0.164 (0.157)	Loss 1.265 (1.237)	Prec@1 64.0 (66.79840087890625)
 * Prec@1 66.993 Best_prec1 0.000Test accuracy= 66.99333190917969 h= 0.832375168800354
===================================== Round 1 =====================================
Testset: 600-------------1
Test-(27): [100/600]	Time 0.173 (0.162)	Loss 1.225 (1.228)	Prec@1 68.0 (67.66336822509766)

Test-(27): [200/600]	Time 0.150 (0.160)	Loss 1.165 (1.226)	Prec@1 74.0 (67.85074615478516)

Test-(27): [300/600]	Time 0.174 (0.159)	Loss 1.165 (1.230)	Prec@1 74.0 (67.50830078125)

Test-(27): [400/600]	Time 0.141 (0.157)	Loss 1.325 (1.230)	Prec@1 58.0 (67.51122283935547)

Test-(27): [500/600]	Time 0.178 (0.157)	Loss 1.305 (1.230)	Prec@1 60.0 (67.51297760009766)
 * Prec@1 67.490 Best_prec1 66.993Test accuracy= 67.48999786376953 h= 0.8267345428466797
===================================== Round 2 =====================================
Testset: 600-------------2
Test-(27): [100/600]	Time 0.162 (0.168)	Loss 1.325 (1.217)	Prec@1 58.0 (68.81188201904297)

Test-(27): [200/600]	Time 0.206 (0.165)	Loss 1.125 (1.222)	Prec@1 78.0 (68.26866149902344)

Test-(27): [300/600]	Time 0.139 (0.162)	Loss 1.325 (1.226)	Prec@1 58.0 (67.90697479248047)

Test-(27): [400/600]	Time 0.246 (0.161)	Loss 1.325 (1.231)	Prec@1 58.0 (67.35660552978516)

Test-(27): [500/600]	Time 0.175 (0.160)	Loss 1.465 (1.231)	Prec@1 44.0 (67.37325286865234)
 * Prec@1 67.553 Best_prec1 67.490Test accuracy= 67.55333709716797 h= 0.8446130752563477
===================================== Round 3 =====================================
Testset: 600-------------3
Test-(27): [100/600]	Time 0.136 (0.171)	Loss 1.385 (1.224)	Prec@1 52.0 (68.03960418701172)

Test-(27): [200/600]	Time 0.162 (0.164)	Loss 1.205 (1.231)	Prec@1 70.0 (67.4328384399414)

Test-(27): [300/600]	Time 0.140 (0.163)	Loss 1.245 (1.229)	Prec@1 66.0 (67.57474517822266)

Test-(27): [400/600]	Time 0.126 (0.161)	Loss 1.325 (1.226)	Prec@1 58.0 (67.86034393310547)

Test-(27): [500/600]	Time 0.161 (0.160)	Loss 1.125 (1.232)	Prec@1 78.0 (67.32534790039062)
 * Prec@1 67.133 Best_prec1 67.553Test accuracy= 67.13333129882812 h= 0.8295167684555054
===================================== Round 4 =====================================
Testset: 600-------------4
Test-(27): [100/600]	Time 0.171 (0.169)	Loss 1.245 (1.254)	Prec@1 66.0 (65.1089096069336)

Test-(27): [200/600]	Time 0.133 (0.165)	Loss 1.185 (1.249)	Prec@1 72.0 (65.55223846435547)

Test-(27): [300/600]	Time 0.139 (0.162)	Loss 1.365 (1.250)	Prec@1 54.0 (65.48837280273438)

Test-(27): [400/600]	Time 0.141 (0.161)	Loss 1.305 (1.251)	Prec@1 60.0 (65.4314193725586)

Test-(27): [500/600]	Time 0.151 (0.161)	Loss 1.085 (1.244)	Prec@1 82.0 (66.10379028320312)
 * Prec@1 66.073 Best_prec1 67.553Test accuracy= 66.07333374023438 h= 0.8557495474815369

Aver_accuracy= 67.04866790771484 Aver_h= 0.8377978205680847
Namespace(arch='DN_X', config='../models/architectures/configs/DN7_DA.yaml', dengine=True, refit_dengine=True, dataset_dir='../dataset/StanfordDogs/', data_name='test', mode='test', resume='../results/models/dn7da_stanfordogs_5_5_k3_epoch_27.pth.tar', epochs=1, ngpu=1, print_freq=100, outf='../results/DN_X_test_DN4_5_Way_5_Shot_K5')=> loaded checkpoint '../results/models/dn7da_stanfordogs_5_5_k3_epoch_27.pth.tar' (epoch 27)
DN_X(
  (BACKBONE): SevenLayer_64F(
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.2, inplace=True)
      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (6): LeakyReLU(negative_slope=0.2, inplace=True)
      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (10): LeakyReLU(negative_slope=0.2, inplace=True)
      (11): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (13): LeakyReLU(negative_slope=0.2, inplace=True)
      (14): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (15): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (16): LeakyReLU(negative_slope=0.2, inplace=True)
      (17): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (18): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (19): LeakyReLU(negative_slope=0.2, inplace=True)
      (20): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (21): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (22): LeakyReLU(negative_slope=0.2, inplace=True)
      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (criterion): CrossEntropyLoss()
  )
  (DE): RandomForestHead()
)
Trainset: 10000
Valset: 600
Testset: 600
===================================== Round 0 =====================================
Testset: 600-------------0
Test-(27): [100/600]	Time 0.132 (0.165)	Loss 1.125 (1.243)	Prec@1 78.0 (66.1980209350586)

Test-(27): [200/600]	Time 0.148 (0.164)	Loss 1.405 (1.230)	Prec@1 50.0 (67.49253845214844)

Test-(27): [300/600]	Time 0.153 (0.163)	Loss 1.185 (1.235)	Prec@1 72.0 (66.93687438964844)

Test-(27): [400/600]	Time 0.128 (0.161)	Loss 1.285 (1.239)	Prec@1 62.0 (66.5685806274414)

Test-(27): [500/600]	Time 0.142 (0.161)	Loss 1.345 (1.236)	Prec@1 56.0 (66.83832550048828)
 * Prec@1 66.810 Best_prec1 0.000Test accuracy= 66.80999755859375 h= 0.8242018818855286
===================================== Round 1 =====================================
Testset: 600-------------1
Test-(27): [100/600]	Time 0.159 (0.176)	Loss 1.025 (1.240)	Prec@1 88.0 (66.47525024414062)

Test-(27): [200/600]	Time 0.176 (0.170)	Loss 1.065 (1.235)	Prec@1 84.0 (67.0248794555664)

Test-(27): [300/600]	Time 0.217 (0.166)	Loss 1.185 (1.232)	Prec@1 72.0 (67.2491683959961)

Test-(27): [400/600]	Time 0.196 (0.165)	Loss 1.085 (1.231)	Prec@1 82.0 (67.41645812988281)

Test-(27): [500/600]	Time 0.164 (0.164)	Loss 1.285 (1.232)	Prec@1 62.0 (67.29341125488281)
 * Prec@1 67.443 Best_prec1 66.810Test accuracy= 67.4433364868164 h= 0.7920087575912476
===================================== Round 2 =====================================
Testset: 600-------------2
Test-(27): [100/600]	Time 0.129 (0.171)	Loss 1.225 (1.217)	Prec@1 68.0 (68.75247192382812)

Test-(27): [200/600]	Time 0.145 (0.162)	Loss 1.325 (1.218)	Prec@1 58.0 (68.66666412353516)

Test-(27): [300/600]	Time 0.184 (0.161)	Loss 1.325 (1.222)	Prec@1 58.0 (68.33222198486328)

Test-(27): [400/600]	Time 0.149 (0.160)	Loss 1.205 (1.222)	Prec@1 70.0 (68.31919860839844)

Test-(27): [500/600]	Time 0.119 (0.160)	Loss 1.145 (1.221)	Prec@1 76.0 (68.37126159667969)
 * Prec@1 68.120 Best_prec1 67.443Test accuracy= 68.12000274658203 h= 0.8352543115615845
===================================== Round 3 =====================================
Testset: 600-------------3
Test-(27): [100/600]	Time 0.149 (0.174)	Loss 0.905 (1.214)	Prec@1 100.0 (69.12871551513672)

Test-(27): [200/600]	Time 0.161 (0.165)	Loss 1.345 (1.221)	Prec@1 56.0 (68.3880615234375)

Test-(27): [300/600]	Time 0.169 (0.163)	Loss 1.325 (1.227)	Prec@1 58.0 (67.8006591796875)

Test-(27): [400/600]	Time 0.151 (0.162)	Loss 1.205 (1.227)	Prec@1 70.0 (67.7955093383789)

Test-(27): [500/600]	Time 0.198 (0.162)	Loss 1.245 (1.230)	Prec@1 66.0 (67.44111633300781)
 * Prec@1 67.710 Best_prec1 68.120Test accuracy= 67.70999908447266 h= 0.8419740200042725
===================================== Round 4 =====================================
Testset: 600-------------4
Test-(27): [100/600]	Time 0.149 (0.170)	Loss 1.265 (1.246)	Prec@1 64.0 (65.90098571777344)

Test-(27): [200/600]	Time 0.181 (0.164)	Loss 1.205 (1.236)	Prec@1 70.0 (66.90547180175781)

Test-(27): [300/600]	Time 0.155 (0.161)	Loss 1.305 (1.233)	Prec@1 60.0 (67.22258758544922)

Test-(27): [400/600]	Time 0.173 (0.160)	Loss 1.165 (1.234)	Prec@1 74.0 (67.09725189208984)

Test-(27): [500/600]	Time 0.150 (0.160)	Loss 1.265 (1.235)	Prec@1 64.0 (67.02594757080078)
 * Prec@1 67.287 Best_prec1 68.120Test accuracy= 67.28666687011719 h= 0.8358665704727173

Aver_accuracy= 67.4739990234375 Aver_h= 0.82586110830307
Namespace(arch='DN_X', config='../models/architectures/configs/DN7_DA.yaml', dengine=True, refit_dengine=False, dataset_dir='../dataset/StanfordDogs/', data_name='test', mode='test', resume='../results/models/dn7da_stanfordogs_5_5_k3_epoch_27.pth.tar', epochs=1, ngpu=1, print_freq=100, outf='../results/DN_X_test_DN4_5_Way_5_Shot_K5')=> loaded checkpoint '../results/models/dn7da_stanfordogs_5_5_k3_epoch_27.pth.tar' (epoch 27)
DN_X(
  (BACKBONE): SevenLayer_64F(
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.2, inplace=True)
      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (6): LeakyReLU(negative_slope=0.2, inplace=True)
      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (10): LeakyReLU(negative_slope=0.2, inplace=True)
      (11): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (13): LeakyReLU(negative_slope=0.2, inplace=True)
      (14): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (15): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (16): LeakyReLU(negative_slope=0.2, inplace=True)
      (17): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (18): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (19): LeakyReLU(negative_slope=0.2, inplace=True)
      (20): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (21): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (22): LeakyReLU(negative_slope=0.2, inplace=True)
      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (criterion): CrossEntropyLoss()
  )
  (DE): RandomForestHead()
)
Trainset: 10000
Valset: 600
Testset: 600
===================================== Round 0 =====================================
Testset: 600-------------0
Test-(27): [100/600]	Time 0.142 (0.168)	Loss 1.705 (1.705)	Prec@1 20.0 (20.0)

Test-(27): [200/600]	Time 0.124 (0.163)	Loss 1.705 (1.705)	Prec@1 20.0 (20.0)
